"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.moveToActive = void 0;
const content = `--[[
  Move next job to be processed to active, lock it and fetch its data. The job
  may be delayed, in that case we need to move it to the delayed set instead.
  This operation guarantees that the worker owns the job during the lock
  expiration time. The worker is responsible of keeping the lock fresh
  so that no other worker picks this job again.
  Input:
      KEYS[1] wait key
      KEYS[2] active key
      KEYS[3] prioritized key
      KEYS[4] stream events key
      KEYS[5] stalled key
      -- Rate limiting
      KEYS[6] rate limiter key
      KEYS[7] delayed key
      -- Promote delayed jobs
      KEYS[8] paused key
      KEYS[9] meta key
      KEYS[10] pc priority counter
      -- Marker
      KEYS[11] marker key
      -- Arguments
      ARGV[1] key prefix
      ARGV[2] timestamp
      ARGV[3] options
]]
local rcall = redis.call
local waitKey = KEYS[1]
local activeKey = KEYS[2]
local rateLimiterKey = KEYS[6]
local delayedKey = KEYS[7]
local prefixKey = ARGV[1]
local timestamp = tonumber(ARGV[2])
local opts = cmsgpack.unpack(ARGV[3])
-- Includes
--[[
  Function to get current rate limit ttl.
]]
local function getRateLimitTTL(maxJobs, rateLimiterKey)
  if maxJobs and maxJobs <= tonumber(rcall("GET", rateLimiterKey) or 0) then
    local pttl = rcall("PTTL", rateLimiterKey)
    if pttl == 0 then
      rcall("DEL", rateLimiterKey)
    end
    if pttl > 0 then
      return pttl
    end
  end
  return 0
end
--[[
  Function to check for the meta.paused key to decide if we are paused or not
  (since an empty list and !EXISTS are not really the same).
]]
local function getTargetQueueList(queueMetaKey, activeKey, waitKey, pausedKey)
  local queueAttributes = rcall("HMGET", queueMetaKey, "paused", "concurrency")
  if queueAttributes[1] then
    return pausedKey, true
  else
    if queueAttributes[2] then
      local activeCount = rcall("LLEN", activeKey)
      if activeCount >= tonumber(queueAttributes[2]) then
        return waitKey, true
      else
        return waitKey, false
      end
    end
  end
  return waitKey, false
end
--[[
  Updates the delay set, by moving delayed jobs that should
  be processed now to "wait".
    Events:
      'waiting'
]]
-- Includes
--[[
  Function to add job in target list and add marker if needed.
]]
-- Includes
--[[
  Add marker if needed when a job is available.
]]
local function addBaseMarkerIfNeeded(markerKey, isPausedOrMaxed)
  if not isPausedOrMaxed then
    rcall("ZADD", markerKey, 0, "0")
  end  
end
local function addJobInTargetList(targetKey, markerKey, pushCmd, isPausedOrMaxed, jobId)
  rcall(pushCmd, targetKey, jobId)
  addBaseMarkerIfNeeded(markerKey, isPausedOrMaxed)
end
--[[
  Function to add job considering priority.
]]
-- Includes
local function addJobWithPriority(markerKey, prioritizedKey, priority, jobId,
    priorityCounterKey, isPausedOrMaxed, groupId)
  local prioCounter
  if groupId then
    prioCounter = rcall("HINCRBY", priorityCounterKey, groupId, 1)
  else
    prioCounter = rcall("INCR", priorityCounterKey)
  end
  local score = priority * 0x100000000 + prioCounter % 0x100000000
  rcall("ZADD", prioritizedKey, score, jobId)
  addBaseMarkerIfNeeded(markerKey, isPausedOrMaxed)
end
-- Includes
--[[
  Function to push back job considering priority in front of same prioritized jobs.
]]
local function pushBackJobWithPriority(prioritizedKey, priority, jobId)
  -- in order to put it at front of same prioritized jobs
  -- we consider prioritized counter as 0
  local score = priority * 0x100000000
  rcall("ZADD", prioritizedKey, score, jobId)
end
local function moveJobToTargetGroup(pushCmd, prefixKey, groupKey, groupId, jobId, markerKey, priority,
    isPaused, pushBack)
    if priority == 0 then
        addJobInTargetList(groupKey, markerKey, pushCmd, isPaused, jobId)
    elseif pushBack then
        pushBackJobWithPriority(groupKey .. ":p", priority, jobId)
    else
        addJobWithPriority(markerKey, groupKey .. ":p", priority, jobId,
            prefixKey .. "groups:pc", isPaused, groupId)
    end
end
local function addToGroup(lifo, prefixKey, groupId, jobId, markerKey, priority, isPaused, pushBack)
    local groupKey = prefixKey .. 'groups:' .. groupId
    local pushCmd = lifo and 'RPUSH' or 'LPUSH';
    --if group is paused we do not need to check for rate limit
    if rcall("ZSCORE", prefixKey .. 'groups:paused', groupId) ~= false then
        -- set isPaused as true in order to avoid adding marker
        moveJobToTargetGroup(pushCmd, prefixKey, groupKey, groupId, jobId, markerKey, priority, true, pushBack)
    else
        -- Has this group reached maximum concurrency?
        local hasReachedMaxConcurrency = rcall("ZSCORE", prefixKey .. 'groups:max', groupId) ~= false
        -- Is group rate limited?
        local groupRateLimitKey = groupKey .. ':limit'
        local jobCounter = tonumber(rcall("GET", groupRateLimitKey))
        local isRateLimited = jobCounter and jobCounter >= 999999
        if hasReachedMaxConcurrency or isRateLimited then
            -- set isPaused as true in order to avoid adding marker
            moveJobToTargetGroup(pushCmd, prefixKey, groupKey, groupId, jobId, markerKey, priority, true, pushBack)
        else
            moveJobToTargetGroup(pushCmd, prefixKey, groupKey, groupId, jobId, markerKey, priority, isPaused, pushBack)
            local groupsKey = prefixKey .. 'groups'
            local highscore = rcall("ZREVRANGE", groupsKey, 0, 0,
                "withscores")[2] or 0
            rcall("ZADD", groupsKey, highscore + 1, groupId)
        end
    end
end
-- Try to get as much as 1000 jobs at once, and returns the nextTimestamp if
-- there are more delayed jobs to process.
local function promoteDelayedJobs(delayedKey, markerKey, targetKey, prioritizedKey,
                                  eventStreamKey, prefix, timestamp, priorityCounterKey, paused)
    local jobs = rcall("ZRANGEBYSCORE", delayedKey, 0, (timestamp + 1) * 0x1000 - 1, "LIMIT", 0, 1000)
    if (#jobs > 0) then
        rcall("ZREM", delayedKey, unpack(jobs))
        for _, jobId in ipairs(jobs) do
            local jobKey = prefix .. jobId
            local jobAttributes = rcall("HMGET", jobKey, "priority", "gid")
            local priority = tonumber(jobAttributes[1]) or 0
            -- Standard or priority add
            if jobAttributes[2] then
                addToGroup(false, prefix, jobAttributes[2], jobId, markerKey,
                    priority, paused, false)
            elseif priority == 0 then
                -- LIFO or FIFO
                addJobInTargetList(targetKey, markerKey, "LPUSH", paused, jobId)
            else
                addJobWithPriority(markerKey, prioritizedKey, priority,
                  jobId, priorityCounterKey, paused, nil)
            end
            -- Emit waiting event
            rcall("XADD", eventStreamKey, "*", "event", "waiting", "jobId",
                  jobId, "prev", "delayed")
            rcall("HSET", jobKey, "delay", 0)
        end
    end
end
--[[
  Promote a rate-limited group (if any) so that it is not rate limited anymore
]]
-- Includes
-- Includes
local function isGroupMaxed(prefixKey, groupId)
  return rcall("ZSCORE", prefixKey .. "groups:max", groupId) ~= false
end
--[[
  Reinsert the group with the highest score so that it is moved to the last position
]]
local function reinsertGroupIfNeeded(groupKey, groupsKey, groupId)
  if rcall("LLEN", groupKey) > 0 or rcall("ZCARD", groupKey .. ":p") > 0 then
    local highscore = rcall("ZREVRANGE", groupsKey, 0, 0, "withscores")[2] or 0
    -- Note, this mechanism could keep increasing the score indefinetely.
    -- Score can represent 2^53 integers, so approximatelly 285 years adding 1M jobs/second
    -- before it starts misbehaving.
    rcall("ZADD", groupsKey, highscore + 1, groupId)
    return true
  else
    rcall("HDEL", groupsKey .. ":pc", groupId)
    return false
  end
end
local function reinsertGroupIfNotMaxedOrPaused(prefixKey, groupId)
  if rcall("ZSCORE", prefixKey .. "groups:paused", groupId) == false then
    if not isGroupMaxed(prefixKey, groupId) then
      local groupsKey = prefixKey .. 'groups'
      local groupKey = prefixKey .. 'groups:' .. groupId
      return reinsertGroupIfNeeded(groupKey, groupsKey, groupId)
    end
  end
end
local function promoteRateLimitedGroups(prefixKey, markerKey, timestamp, paused)
    local groupsRateLimitKey = prefixKey .. 'groups:limit'
    local groupIds = rcall("ZPOPMIN", groupsRateLimitKey)
    if #groupIds > 0 then
        -- Is the group really limited?
        local groupRateLimitKey = prefixKey .. 'groups:' .. groupIds[1] ..
                                      ':limit'
        local jobCounter = tonumber(rcall("GET", groupRateLimitKey))
        local groupId = groupIds[1]
        if not jobCounter or jobCounter < 999999 then
            -- Group is not rate limited anymore so we promote it
            if reinsertGroupIfNotMaxedOrPaused(prefixKey, groupId) then
                addBaseMarkerIfNeeded(markerKey, paused)
                return 0, 0
            else
                return promoteRateLimitedGroups(prefixKey, markerKey, timestamp, paused)
            end
        else
            -- remove the key manually if ttl is zero to avoid side effects.
            local ttl = tonumber(rcall("PTTL", groupRateLimitKey))
            if ttl == 0 then rcall("DEL", groupRateLimitKey) end
            -- Group is still rate limited, re-add with new score
            local nextTimestamp = timestamp + ttl
            rcall("ZADD", groupsRateLimitKey, nextTimestamp, groupId)
            return nextTimestamp, ttl
        end
    end
    return 0, 0
end
--[[
    Fetches the next job to be processed and locks it atomically for the specified worker.
    If there are jobs but they are delayed or rate limit, this function will return the next
    delayed or rate limited timestamp so that the worker can sleep until that time before
    asking for a new job again.
]]
-- Includes
--[[
  Function to return the next delayed job timestamp.
]]
local function getNextDelayedTimestamp(delayedKey)
  local result = rcall("ZRANGE", delayedKey, 0, 0, "WITHSCORES")
  if #result then
    local nextTimestamp = tonumber(result[2])
    if nextTimestamp ~= nil then 
      return nextTimestamp / 0x1000
    end
  end
end
--[[
  Function to move job from prioritized state to active.
]]
local function moveJobFromPrioritizedToActive(priorityKey, activeKey, priorityCounterKey, groupId)
  local prioritizedJob = rcall("ZPOPMIN", priorityKey)
  if #prioritizedJob > 0 then
    rcall("LPUSH", activeKey, prioritizedJob[1])
    return prioritizedJob[1]
  else
    if groupId then
      rcall("HDEL", priorityCounterKey, groupId)
    else
      rcall("DEL", priorityCounterKey)
    end
  end
end
-- Includes
local function increaseGroupConcurrency(groupsKey, groupId, maxConcurrency, timestamp)
  local count = rcall("HINCRBY", groupsKey .. ':active:count', groupId, 1)
  local localConcurrency = rcall("HGET", groupsKey .. ":concurrency", groupId)
  if count >= tonumber(localConcurrency or maxConcurrency) then
    rcall("ZADD", groupsKey .. ':max', timestamp, groupId)
    rcall("ZREM", groupsKey, groupId)
    return true
  end
end
local function rateLimitGroup(prefixKey, groupId, maxRate, rateDuration,
                              timestamp)
    if maxRate then
        local groupsKey = prefixKey .. 'groups'
        local groupKey = groupsKey .. ':' .. groupId
        local groupRateLimitKey = groupKey .. ':limit'
        -- Update limit key for this group, if rate-limited move the group to the rate limited zset
        local jobCounter = tonumber(rcall("INCR", groupRateLimitKey))
        if jobCounter == 1 then
            rcall("PEXPIRE", groupRateLimitKey, rateDuration)
        end
        -- -- check if rate limit hit
        if jobCounter >= maxRate then
            if jobCounter < 999999 then
                -- Set magic number 999999 to mark this group as rate limited
                -- use INCRBY as SET would clean the EXPIRE on the key.
                rcall("INCRBY", groupRateLimitKey, 999999)
            end
            -- Since this group is rate limited, remove it from the groupsKey and
            -- add it to the limit set.
            rcall("ZREM", groupsKey, groupId)
            local groupsRateLimitKey = prefixKey .. 'groups:limit'
            local nextTimestamp = timestamp + rateDuration
            rcall("ZADD", groupsRateLimitKey, nextTimestamp, groupId)
            return true
        end
    end
    return false
end
-- TODO: We are missing the fact that if we get rate limited in "moveToActive" or "moveToFinished" we need
-- to also return the next TTL, so basically taking the min(promoted-TTL, newRateLimitedTTL)
local function moveJobToActiveFromGroup(prefixKey, activeKey, groupMaxConcurrency,
  groupLimit, groupLimitDuration, timestamp)
  local groupsKey = prefixKey .. 'groups'
  local jobId
  -- Try to fetch next group's jobs
  local groupIds = rcall("ZPOPMIN", groupsKey)
  if #groupIds > 0 then
    local groupId = groupIds[1]
    local groupKey = groupsKey .. ':' .. groupId
    jobId = rcall("RPOPLPUSH", groupKey, activeKey)
    if not jobId then
      jobId = moveJobFromPrioritizedToActive(groupKey .. ":p", activeKey, groupKey .. ":pc", groupId)
    end
    -- Handle maxGroupConcurrency
    if groupMaxConcurrency and jobId then
      if increaseGroupConcurrency(groupsKey, groupId, groupMaxConcurrency, timestamp) then
        return jobId
      end
    end
    if groupLimit and jobId then
      if rateLimitGroup(prefixKey, groupId, groupLimit, groupLimitDuration, timestamp) then
        return jobId
      end
    end
    reinsertGroupIfNeeded(groupKey, groupsKey, groupId)
    rcall("SET", prefixKey .. 'groups-lid', groupId)
  end
  return jobId
end
-- Includes
local function prepareJobForProcessing(keyPrefix, rateLimiterKey,
                                       eventStreamKey, jobId, processedOn,
                                       maxJobs, markerKey, options, rateLimitedNextTtl)
    local token = options['token']
    local lockDuration = options['lockDuration']
    local jobKey = keyPrefix .. jobId
    if maxJobs then
        -- Check if we need to perform global rate limiting
        local jobCounter = tonumber(rcall("INCR", rateLimiterKey))
        if jobCounter == 1 then
            local limiterDuration = options['limiter'] and
                                        options['limiter']['duration']
            local integerDuration = math.floor(math.abs(limiterDuration))
            rcall("PEXPIRE", rateLimiterKey, integerDuration)
        end
    end
    local lockKey = jobKey .. ':lock'
    rcall("SET", lockKey, token, "PX", lockDuration)
    local optionalValues = {}
    if options['name'] then
        -- Set "processedBy" field to the worker name
        table.insert(optionalValues, "pb")
        table.insert(optionalValues, opts['name'])
    end
    rcall("XADD", eventStreamKey, "*", "event", "active", "jobId", jobId,
          "prev", "waiting")
    rcall("HMSET", jobKey, "processedOn", processedOn, unpack(optionalValues))
    rcall("HINCRBY", jobKey, "ats", 1)
    -- This is a bit wrong actually, the queue could have jobs that are ratelimited or
    -- have reached max concurrency, so in that case we should not emit this.
    addBaseMarkerIfNeeded(markerKey, false)
    return {rcall("HGETALL", jobKey), jobId, 0, rateLimitedNextTtl or 0} -- get job data
end
-- TODO: parametrize KEYS
local function fetchNextJob(waitKey, activeKey, delayedKey,
                            rateLimiterKey, eventStreamKey, prefixKey, opts,
                            timestamp, markerKey, paused, maxJobs, KEYS,
                            rateLimitedNextTtl, checkDrained)
    -- Check if we move this call outside of the function so that it is only performed once per batch.
    if not rateLimitedNextTtl then
        -- TODO: add default value 0 in rateLimitedNextTtl
        rateLimitedNextTtl = promoteRateLimitedGroups(prefixKey, markerKey, timestamp, paused)
    end
    local jobId = rcall("RPOPLPUSH", waitKey, activeKey)
    -- If jobId is special ID 0:delay, then there is no job to process
    if jobId then
        if string.sub(jobId, 1, 2) == "0:" then
            rcall("LREM", activeKey, 1, jobId)
            jobId = rcall("RPOPLPUSH", waitKey, activeKey)
        end
    end
    if jobId then
        return prepareJobForProcessing(prefixKey, rateLimiterKey, eventStreamKey, jobId, timestamp,
            maxJobs, markerKey, opts, rateLimitedNextTtl)
    else
        jobId = moveJobFromPrioritizedToActive(KEYS[3], activeKey, KEYS[10], nil)
        if jobId then
            return prepareJobForProcessing(prefixKey, rateLimiterKey, eventStreamKey, jobId, timestamp,
                maxJobs, markerKey, opts, rateLimitedNextTtl)
        else
            local groupLimit
            local groupLimitDuration
            local groupMaxConcurrency
            if opts['group'] then
                if opts['group']['limit'] then
                    groupLimit = opts['group']['limit']['max']
                    groupLimitDuration = opts['group']['limit']['duration']
                end
                groupMaxConcurrency = opts['group']['concurrency']
            end
            jobId = moveJobToActiveFromGroup(prefixKey, activeKey, groupMaxConcurrency, groupLimit,
                                            groupLimitDuration, timestamp)
            if jobId then
                return prepareJobForProcessing(prefixKey, rateLimiterKey, eventStreamKey, jobId,
                                                timestamp, maxJobs, markerKey, opts, rateLimitedNextTtl)
            end
        end
    end
    -- Return the timestamp for the next delayed job if any.
    local nextTimestamp = getNextDelayedTimestamp(delayedKey)
    if nextTimestamp ~= nil then
        -- The result is guaranteed to be positive, since the
        -- ZRANGEBYSCORE command would have return a job otherwise.
        return {0, 0, 0, nextTimestamp}
    end
    if checkDrained then
        local waitLen = rcall("LLEN", waitKey)
        if waitLen == 0 then
            local activeLen = rcall("LLEN", activeKey)
            if activeLen == 0 then
                local prioritizedLen = rcall("ZCARD", KEYS[3])
                if prioritizedLen == 0 then
                    rcall("XADD", eventStreamKey, "*", "event", "drained")
                end
            end
        end
    end
    return {0, 0, 0, rateLimitedNextTtl or 0}
end
--[[
KEYS[1] wait key
KEYS[2] active key
KEYS[3] priority key
KEYS[4] stream events key
KEYS[5] stalled key
-- Rate limiting
KEYS[6] rate limiter key
KEYS[7] delayed key
]]
-- Includes
-- TODO: Try to get rid of KEYS and use separate parameters instead.
-- TODO: If queue is paused just return 0, 0, 0, 0
local function moveNextBatch(waitKey, activeKey, delayedKey,
                             rateLimiterKey, streamEventsKey, prefixKey, opts,
                             numJobs, timestamp, markerKey, rateLimiterKey,
                             maxJobs, KEYS)
    local lastResult
    local jobs = {}
    local jobIds = {}
    repeat
        local expireTime = getRateLimitTTL(maxJobs, rateLimiterKey)
        if expireTime > 0 then
            lastResult =  {0, 0, expireTime, 0}
        else
            lastResult = fetchNextJob(waitKey, activeKey, delayedKey,
                rateLimiterKey, streamEventsKey, prefixKey,
                opts, timestamp, markerKey, false,
                maxJobs, KEYS, nil, false)
        end
        numJobs = numJobs - 1
        if lastResult[1] ~= 0 then
            table.insert(jobs, lastResult[1])
            table.insert(jobIds, lastResult[2])
        end
    until numJobs <= 0 or lastResult[1] == 0
    return {jobs, jobIds, lastResult[3], lastResult[4]}
end
local target, isPausedOrMaxed = getTargetQueueList(KEYS[9], KEYS[2], KEYS[1], KEYS[8])
-- Check if there are delayed jobs that we can move to wait.
local markerKey = KEYS[11]
promoteDelayedJobs(delayedKey, markerKey, target, KEYS[3], KEYS[4],
                   prefixKey, timestamp, KEYS[10], isPausedOrMaxed)
local maxJobs = tonumber(opts['limiter'] and opts['limiter']['max'])
local expireTime = getRateLimitTTL(maxJobs, rateLimiterKey)
-- Check if we are rate limited first.
if expireTime > 0 then return {0, 0, expireTime, 0} end
-- paused queue
if isPausedOrMaxed then return {0, 0, 0, 0} end
if opts.batch then
    local rateLimitedNextTtl = promoteRateLimitedGroups(prefixKey, markerKey,
                                                        timestamp, isPausedOrMaxed)
    local batchSize = (opts.batch and opts.batch.size) or 1
    return moveNextBatch(waitKey, activeKey, delayedKey, rateLimiterKey,
                         KEYS[4], prefixKey, opts, batchSize, timestamp, markerKey,
                         rateLimiterKey, maxJobs, KEYS)
else
    return fetchNextJob(waitKey, activeKey, delayedKey, rateLimiterKey,
                        KEYS[4], prefixKey, opts, timestamp, markerKey, false,
                        maxJobs, KEYS, nil, false)
end
`;
exports.moveToActive = {
    name: 'moveToActive',
    content,
    keys: 11,
};
//# sourceMappingURL=moveToActive-11.js.map